{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kqs3M-cUIK1"
      },
      "source": [
        "# Imports and Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PRUPEXHWNtyb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import random\n",
        "import os\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mdlwC4GOPvn",
        "outputId": "879326dd-fb23-42d2-caa5-d0cad1b41bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 5,484 B/129\r                                                                               \rGet:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 14.2 kB/129\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 33.0 kB/129\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,006 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,267 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,797 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,627 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [88.8 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
            "Fetched 30.0 MB in 2s (13.1 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  ncbi-data\n",
            "The following NEW packages will be installed:\n",
            "  ncbi-blast+ ncbi-data\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 15.8 MB of archives.\n",
            "After this operation, 71.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ncbi-data all 6.1.20170106+dfsg1-9 [3,519 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ncbi-blast+ amd64 2.12.0+ds-3build1 [12.3 MB]\n",
            "Fetched 15.8 MB in 2s (9,961 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ncbi-data.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../ncbi-data_6.1.20170106+dfsg1-9_all.deb ...\n",
            "Unpacking ncbi-data (6.1.20170106+dfsg1-9) ...\n",
            "Selecting previously unselected package ncbi-blast+.\n",
            "Preparing to unpack .../ncbi-blast+_2.12.0+ds-3build1_amd64.deb ...\n",
            "Unpacking ncbi-blast+ (2.12.0+ds-3build1) ...\n",
            "Setting up ncbi-data (6.1.20170106+dfsg1-9) ...\n",
            "Setting up ncbi-blast+ (2.12.0+ds-3build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install ncbi-blast+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYwb-XX2ORdZ",
        "outputId": "116f2175-8112-4725-c73d-6e72c69d55db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting numpy (from biopython)\n",
            "  Downloading numpy-2.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Downloading biopython-1.85-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, biopython\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [biopython]\n",
            "\u001b[1A\u001b[2KSuccessfully installed biopython-1.85 numpy-2.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install biopython\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRE81r2zOTf5",
        "outputId": "45f38969-9c5c-4c00-d1fe-eaac256cd867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Colab notebook uses the BLAST+ command-line tools to detect a suspicious junction in a sequencing read.\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Constants\n",
        "BLAST_DB_NAME = 'viral_db'\n",
        "BLAST_OUTPUT_PATH = 'blast_results.tsv'\n",
        "BLAST_DB_TYPE = 'nucl'\n",
        "EVALUE_THRESHOLD = '10'\n",
        "\n",
        "# --- Installation and Database Setup ---\n",
        "\n",
        "def setup_blast_and_database(viral_genome_path, db_name):\n",
        "    \"\"\"\n",
        "    Installs BLAST+ and creates a BLAST database from a viral genome.\n",
        "\n",
        "    Args:\n",
        "        viral_genome_path (str): The path to the multi-FASTA file of viral genomes.\n",
        "        db_name (str): The name for the new BLAST database.\n",
        "    \"\"\"\n",
        "    print(\"--- Setting up BLAST+ and database ---\")\n",
        "\n",
        "    # Check if BLAST+ is installed by running `blastn -version`.\n",
        "    try:\n",
        "        subprocess.run([\"blastn\", \"-version\"], check=True, capture_output=True)\n",
        "        print(\"BLAST+ is already installed.\")\n",
        "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "        print(\"BLAST+ not found. Installing now...\")\n",
        "        # Use conda to install BLAST+\n",
        "        try:\n",
        "            # Install Miniconda first if it's not present\n",
        "            if not os.path.exists(\"miniconda\"):\n",
        "                print(\"Miniconda not found. Installing...\")\n",
        "                subprocess.run(\n",
        "                    \"wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\",\n",
        "                    shell=True, check=True\n",
        "                )\n",
        "                subprocess.run(\"bash miniconda.sh -b -p miniconda\", shell=True, check=True)\n",
        "                os.environ[\"PATH\"] = f\"{os.getcwd()}/miniconda/bin:{os.environ['PATH']}\"\n",
        "                print(\"Miniconda installed.\")\n",
        "\n",
        "            # Install BLAST+ from bioconda channel\n",
        "            subprocess.run(\n",
        "                \"conda install -c bioconda -c conda-forge blast --yes\",\n",
        "                shell=True, check=True\n",
        "            )\n",
        "            print(\"BLAST+ installed successfully.\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error during BLAST+ installation: {e.stderr.decode()}\")\n",
        "            return\n",
        "\n",
        "    # Create the BLAST database.\n",
        "    print(f\"Creating BLAST database '{db_name}' from {viral_genome_path}...\")\n",
        "    try:\n",
        "        command = [\"makeblastdb\", \"-in\", viral_genome_path, \"-dbtype\", BLAST_DB_TYPE, \"-out\", db_name]\n",
        "        subprocess.run(command, check=True, capture_output=True)\n",
        "        print(\"BLAST database created successfully.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error creating BLAST database: {e.stderr.decode()}\")\n",
        "\n",
        "def write_fasta_file(sequences, file_path):\n",
        "    \"\"\"Write sequences to a FASTA file from a list.\"\"\"\n",
        "    with open(file_path, 'w') as f:\n",
        "        for i, sequence in enumerate(sequences):\n",
        "            f.write(f'>{i}\\n')\n",
        "            f.write(f'{sequence}\\n')\n",
        "\n",
        "def run_blast(query_path, db_name, output_path, evalue):\n",
        "    \"\"\"Run BLAST and save the results to an output file.\"\"\"\n",
        "    subprocess.run([\n",
        "        'blastn',\n",
        "        '-query', query_path,\n",
        "        '-db', db_name,\n",
        "        '-out', output_path,\n",
        "        '-outfmt', '6',  # Tabular output\n",
        "        '-max_hsps', '1',\n",
        "        '-evalue', evalue,\n",
        "        '-max_target_seqs', '1',\n",
        "        '-word_size', '7' # A smaller word size to find matches in simple sequences\n",
        "    ], check=True)\n",
        "\n",
        "def parse_blast_results(blast_output_path):\n",
        "    \"\"\"Parse BLAST tabular results and return a DataFrame.\"\"\"\n",
        "    columns = [\n",
        "        \"query_id\", \"subject_id\", \"% identity\", \"alignment_length\", \"mismatches\",\n",
        "        \"gap_openings\", \"q_start\", \"q_end\", \"s_start\", \"s_end\", \"evalue\", \"bit_score\"\n",
        "    ]\n",
        "    if os.path.getsize(blast_output_path) == 0:\n",
        "        return pd.DataFrame(columns=columns)\n",
        "\n",
        "    blast_df = pd.read_csv(blast_output_path, sep='\\t', header=None, names=columns)\n",
        "    return blast_df\n",
        "\n",
        "def detect_suspicious_junction(df, min_non_matching_bases=30):\n",
        "    \"\"\"\n",
        "    Identifies suspicious reads in a BLAST results DataFrame.\n",
        "    \"\"\"\n",
        "    suspicious_reads = []\n",
        "\n",
        "    # We need to know the original read length. Since we can't get it from BLAST output alone,\n",
        "    # we'll use a hardcoded value for this dummy example. In a real pipeline,\n",
        "    # you would pass in the read lengths from your FASTQ file.\n",
        "    read_lengths = {\n",
        "        0: 200, # This is a dummy value for the length of our natural read\n",
        "        1: 200, # This is a dummy value for the length of our chimeric read\n",
        "    }\n",
        "\n",
        "    if df.empty:\n",
        "      return suspicious_reads\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        query_id = int(row['query_id'])\n",
        "        alignment_len = row['alignment_length']\n",
        "        identity = row['% identity']\n",
        "\n",
        "        print(f\"Query ID: {query_id}, Alignment Length: {alignment_len}, Identity: {identity}\")\n",
        "\n",
        "        # Get the original read length from our lookup.\n",
        "        original_read_len = read_lengths.get(query_id, 0)\n",
        "\n",
        "        if original_read_len > 0:\n",
        "            non_matching_part = original_read_len - alignment_len\n",
        "\n",
        "            # Apply the criteria from the article.\n",
        "            if identity > 95 and non_matching_part > min_non_matching_bases:\n",
        "                suspicious_reads.append(query_id)\n",
        "\n",
        "    return suspicious_reads\n",
        "\n",
        "# --- Dummy Input Data and Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Dummy data\n",
        "    viral_genome_file = \"partial_genome.fasta\"\n",
        "    partial_viral_genome = \"GATTGTGAGCGATTTGCGTGCGTGCATCCCGCTTCACTGATCTCTTGTTAGATCTTTTTGTAATCTAAACTTTATAAAAACATCCACTCCCTGTAATCTATGCTTGTGGGCGTAGATTTTTCATAGTGGTGTTTATATTCATTTCTGCTGTTAACAGCTTTCAGCCAGGGACGTGTTGTATCCTAGGCAGTGGCCCGCCCATAGGTCACAATGTCGAAGATCAACAAATACGGTCTCGAACTACACTGGGCTCCAGAATTTCCATGGATGTTTGAGGACGCAGAGGAGAAGTTGGATAACCCTAGTAGTTCAGAGGTGGATATGATTTGCTCCACCACTGCGCAAAAGCTGGAAACAGACGGAATTTGTCCTGAAAATCATGTGATGGTGGATTGTCGCCGACTTCTTAAACAAGAGTGTTGTGTGCAGTCTAGCCTAATACGTGAAATTGTTATGAATGCAAGTCCATATGATTTGGAGGTGCTACTTCAAGATGCTT\"\n",
        "\n",
        "    with open(viral_genome_file, \"w\") as f:\n",
        "        f.write(\">partial_virus\\n\")\n",
        "        f.write(partial_viral_genome + \"\\n\")\n",
        "\n",
        "    setup_blast_and_database(viral_genome_file, BLAST_DB_NAME)\n",
        "\n",
        "    # Example 1: A natural, non-chimeric read.\n",
        "    natural_read = partial_viral_genome[100:300]\n",
        "\n",
        "    # Example 2: A chimeric read with an engineered insert.\n",
        "    chimeric_read = partial_viral_genome[100:260] + (\"T\" * 40)\n",
        "\n",
        "    # Write dummy reads to a query FASTA file.\n",
        "    query_sequences = [natural_read, chimeric_read]\n",
        "    query_fasta_file = \"query_reads.fasta\"\n",
        "    write_fasta_file(query_sequences, query_fasta_file)\n",
        "\n",
        "    print(\"\\n--- Running BLAST ---\")\n",
        "    run_blast(query_fasta_file, BLAST_DB_NAME, BLAST_OUTPUT_PATH, EVALUE_THRESHOLD)\n",
        "\n",
        "    print(\"\\n--- Parsing BLAST Results and Detecting Suspicious Reads ---\")\n",
        "    blast_df = parse_blast_results(BLAST_OUTPUT_PATH)\n",
        "\n",
        "    if not blast_df.empty:\n",
        "      print(\"BLAST results:\")\n",
        "      print(blast_df)\n",
        "\n",
        "      suspicious_reads = detect_suspicious_junction(blast_df)\n",
        "\n",
        "      print(\"\\n--- Final Results ---\")\n",
        "      if suspicious_reads:\n",
        "          print(\"The following query IDs are suspicious:\")\n",
        "          for read_id in suspicious_reads:\n",
        "              print(f\"  - Read ID: {read_id}\")\n",
        "      else:\n",
        "          print(\"No suspicious reads were detected.\")\n",
        "\n",
        "    else:\n",
        "      print(\"No BLAST results were found.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Setting up BLAST+ and database ---\n",
            "BLAST+ is already installed.\n",
            "Creating BLAST database 'viral_db' from partial_genome.fasta...\n",
            "BLAST database created successfully.\n",
            "\n",
            "--- Running BLAST ---\n",
            "\n",
            "--- Parsing BLAST Results and Detecting Suspicious Reads ---\n",
            "BLAST results:\n",
            "   query_id     subject_id  % identity  alignment_length  mismatches  \\\n",
            "0         0  partial_virus       100.0               200           0   \n",
            "1         1  partial_virus       100.0               161           0   \n",
            "\n",
            "   gap_openings  q_start  q_end  s_start  s_end         evalue  bit_score  \n",
            "0             0        1    200      101    300  2.830000e-107        370  \n",
            "1             0        1    161      101    261   1.350000e-85        298  \n",
            "Query ID: 0, Alignment Length: 200, Identity: 100.0\n",
            "Query ID: 1, Alignment Length: 161, Identity: 100.0\n",
            "\n",
            "--- Final Results ---\n",
            "The following query IDs are suspicious:\n",
            "  - Read ID: 1\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iP0A5OTSqSX",
        "outputId": "70a61d4f-33b2-4f26-933c-dba80af9a522"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}