# Notes for Methods Section

This document outlines the methodology used in the virus engineering detection project.

## 1. Dataset Construction and Preprocessing

The primary dataset was constructed from two main sources: natural viral genomes and synthetically engineered sequences.

### 1.1. Data Sourcing

- **Natural Viral Genomes:** A collection of viral genomes was downloaded from the National Center for Biotechnology Information (NCBI) database using the Entrez API. The specific accession numbers and virus names are detailed in the `notebooks/Research_Sprint_Viral_Genome_Downloader.ipynb` notebook.
- **Engineered Viral Genomes:** Engineered sequences were synthetically generated by applying various modifications to the natural genomes. 

### 1.2. Synthetic Engineering Methods

Several methods were used to simulate genomic engineering, including:
- `region_deletion`: Deletion of a contiguous segment of the genome.
- `gfp_insertion`, `tn5_insertion`, `crispr_insertion`: Insertion of specific genetic markers or constructs.
- `region_inversion` / `region_duplication`: Inversion or duplication of a genomic region.
- `random_substitution`: Replacement of a genomic region with a random sequence.

### 1.3. Dataset Generation

- The final datasets for training and testing were generated by creating segments of a fixed-length distribution (e.g., mean 187bp) from both natural and engineered source genomes.
- A key preprocessing step (`utils/virus_data_processor.py`) assigns a label of `1` (engineered) or `0` (natural) to each segment. 
- **Labeling Correction:** An important correction was identified in the labeling logic. The initial logic incorrectly handled deletions and sequences with unknown modification sites. The process was refined to:
    1. Define the engineered site of a deletion as a 1-base-pair "scar" at the junction point.
    2. Require a non-zero overlap (`overlap > 0`) with a known modification site for a segment to be labeled as engineered, removing a condition that incorrectly labeled entire genomes.

## 2. Baseline Models

Three baseline models were implemented and evaluated.

### 2.1. K-mer Baseline

- **Features:** Sequences were converted into numerical vectors based on k-mer frequencies (k=6). A `CountVectorizer` was used to create a high-dimensional feature space based on these counts.
- **Model:** A `Logistic Regression` model from the scikit-learn library was trained on these k-mer features.
- **Imbalance Handling:** Class imbalance was addressed directly during training by setting the `class_weight='balanced'` parameter, which penalizes misclassifications of the minority (engineered) class more heavily.

### 2.2. BLAST Baseline

- **Methodology:** This approach uses sequence alignment to detect engineering. A BLAST database was constructed from all natural sequences in the training set. Each sequence in the test set was then searched against this database using `blastn`.
- **Classification Logic:** A test sequence was classified as engineered if its best alignment had high sequence identity (>95%) but failed to cover a significant portion of its length (e.g., >30 non-matching bases), suggesting a synthetic insertion.
- **Optimization:** To improve performance on multi-core systems, the `blastn` command was configured to utilize multiple CPU threads (`-num_threads`).

### 2.3. Convolutional Neural Network (CNN) Baseline

- **Input Representation:** DNA sequences were one-hot encoded into a 4-channel matrix (for A, C, G, T) before being fed to the network.
- **Architecture:** A standard 1D CNN architecture was used, involving convolutional layers to act as motif detectors, followed by pooling and fully-connected layers for classification.
- **Loss Function:** The model was trained using `CrossEntropyLoss`.
- **Imbalance Handling:** The current implementation does not apply class weights to the loss function during training. Instead, it compensates for class imbalance post-training by adjusting the decision threshold (default 0.5) to a new value that maximizes the F1-score on the test set. It is noted that enabling class weighting during training is a potential area for improvement.

## 3. Evaluation

- **Primary Metric:** The F1-score was used as the primary evaluation metric due to the significant class imbalance inherent in the dataset. The F1-score provides a balance between precision and recall, making it more informative than accuracy for this task.
- **Other Metrics:** Precision, recall, and accuracy were also calculated for a comprehensive performance overview.
- **Analysis:** A plotting notebook (`notebooks/plotting_experiments_v2.ipynb`) was created to visualize and compare the F1 scores of the different models, both overall and for each specific engineering method.
